== Required packages ==
To run mc_wald_xqtl.r you need 2 packages that are not included in a default R installation:
multicore
bigmemory

To run some of the post-analysis scripts you will also need
biganalytics

These can be installed within R, e.g.
install.packages('multicore')



== Overview ==

To do an xQTL analysis you need 2 data matrices (optionally a third): 'phenotypes', 'genotypes', and 'covars'. All matrices should have the same number of rows with individuals in the same order and should have data in a numeric format. More details for each matrix are provided below.

If there are no covariates, you must still define the 'covars' variable and set it to NULL.

It is helpful (but not required) to name the rows and columns of all three data matrices.


Once you have the data in the correct format, you have two options for running the analysis:

== Running from the command line ==
To run on the command line you should first save your data in .RData format, e.g.:

save(genotypes, phenotypes, covars, file='my_eqtl.RData')

Then from the command line call

Rscript mc_wald_xqtl.r my_eqtl 32 5000

The first argument ('my_eqtl') is the prefix of the .RData file containing the data and will be used to name the output files.

The second argument is the number of cores to use.

The third argument is the number of genotypes to process at a time. If not specified then the default (5000) is used.

For more details on these arguments see the section below.

== Running from R ==
If you prefer you can run the analysis from within R, or from another R script using the source command. First you must define a few variables:

PREFIX : If we need to load data we will look for the file named PREFIX.RData. Before looking for PREFIX.RData the script will check to see if the variables 'phenotypes', 'genotypes', and 'covars' are defined. The PREFIX is also used to name the output files.

MC.CORES : The number of cores to use

BATCH.SIZE : (Optional, default 5000) The number of genotypes to process at one time.

To start the analysis:
source('mc_wald_xqtl.r')

For more details on the arguments see the section below.

=== genotypes ===
The genotypes matrix has 1 row per sample (individual) and as many columns as there are genotypes (usually, but not necessarily, SNPs). Any type of coding can be used (e.g. additive, dominance, or something else you made up).

If you have a large number of genotypes and are using integer coding, you can save memory (and loading time) by telling R that genotypes is an integer matrix:
mode(genotypes) <- 'integer'

Currently the script does NOT handle missing values.

=== phenotypes ===
The phenotypes matrix has 1 row per sample (individual) and as many columns as there are phenotypes, e.g. gene expression measured by array or RNAseq.

The phenotype matrix should be numeric.

Ideally each column is normally distributed, but this is not required.

=== covars ===
The covars matrix has 1 row per sample (individual) and as many columns as their are covariates. Note that factor-type covariates must be converted to multiple binary variables. You can use the model.matrix() function to do this.

For example, suppose you have a data frame 'samples' with a column 'eth' that contains ancestry information: 

1	Afr
2	Cau
3	Cau
4	Afr
5	Asn
6	Afr

Then calling "model.matrix(~ 0 + eth, data=samples)" will produce a matrix with two columns (1 less than the number of factors):

	ethAfr	ethCau
1	1		0
2	0		1
3	0		1
4	1		0
5	0		0
6	1		0			



== Script arguments ==
Whether run from the command line or from within R, there are 2 required arguments and one optional argument, described here.


=== PREFIX ===

This is the first argument when running from the command line and is used to identify both the input and output files.

The script will check to see if the data is available (i.e. if the variables 'genotypes', 'phenotypes', and 'covars' are defined), and if not, it will attempt to load the file PREFIX.RData which should contain the three variables as described previously in this document. Note that even if there are no covariates, 'covars' must be defined and should be set to NULL.

Two output files will be generated when the analysis starts:

PREFIX_pvals.desc
PREFIX_pvals.bin

These two files store a file-backed big.matrix using the 'bigmemory' package. We will not go into details about the bigmemory package/format here except to say that once the files exist they can be loaded into R like so:

require('bigmemory')
pvals <- attach.big.matrix('PREFIX_pvals.desc')


=== MC.CORES ===

This is the second argument when running from the command line and indicates the number of cores to use. For most xQTL analyses you will want multiple cores! More is better but this shouldn't be more than the number of CPUs available on the machine and keep in mind that other people may need to use CPUs as well.

With some allowance for overhead, the increase in computation speed should be approximately linear with the number of cores (twice as many cores = twice as fast)

This won't be the case if you specify more cores than  there are CPUs available, either because they don't exist or because they are already being used! 


=== BATCH.SIZE ===

This is the third (optional) argument when running from the command line. The appropriate batch size depends both on the size of the data you are working with and the memory capabilities of the machine you are working on.

The analysis processes one batch at a time, calculates all of the p-values for that batch, and then writes those values to the p-value matrix (stored on disk).

There are several reasons why we perform the analysis in batches:

1. Memory usage. For a typical analysis, say 500,000 genotypes and 10,000 phenotypes (gene expression) the output is 500,000 x 10,000 = 5,000,000,000 = 5e9 = 5 billion pvalues. Each p-value take 8 bytes to store so the entire matrix is 4e10 bytes = 39062500 KiB = 38147 MiB = 37.25 GiB. So you may run into problems where you don't have enough RAM, especially as the number of phenotypes/genotypes increases.

2. Resuming after interruption. Even if you do have enough memory to store gigabytes of p-values, the calculations are time consuming and if something goes wrong everything stored in memory will be lost. The smaller the batch size, the more often we save. Note that saving TOO often will hurt performance due to the overhead associated with creating multiple threads/collecting the results. At a minimum, the batch size should be set to several multiples of MC.CORES to ensure that we take full advantage of parallel processing.

Typically I try to set the batch size so that each batch takes 5-10 minutes. The length of time it takes to process one batch depends on the size of the data set (number of individuals & phenotypes) and the number of cores used in addition to the batch size.